import requests
from requests.auth import HTTPDigestAuth
import datetime
import json
import settings
import re
import time

# Settings:
search_url = settings.AZURE_ENDPOINT
subscription_key = settings.AZURE_SUBSCRIPTION_KEY
search_term = settings.SEARCH_TERM
newsserviceUrl = settings.NEWSSERVICE_ENDPOINT
newsserviceApikey = settings.NEWSSERVICE_APIKEY
freshness = settings.NEWSSERVICE_FRESHNESS
lang = settings.NEWSSERVICE_LANG
region = settings.NEWSSERVICE_REGION
mkt = lang+"-"+region

cvePattern = re.compile(r"(CVE-\d+-\d+)", re.IGNORECASE)
blacklistPattern = re.compile(r"nvd.nist.gov/vuln", re.IGNORECASE)

# Counters:
found=0
stored=0
duplicates=0
noCveFound=0
blacklisted=0
responseCodes={}

# uppercase and remove duplicates:
def clean_cves(cves):
  new_cves = [cve.upper() for cve in cves]
  return list(dict.fromkeys(new_cves))


def process_item(item):
  global found, stored, noCveFound, blacklisted, responseCodes, duplicates

  if (blacklistPattern.search(item["url"])):
    blacklisted += 1
    return

  cves = cvePattern.findall(
    item["description"] + item["name"] + item["url"]
  )
  scrapedCves = scrapeCves(item["url"])
  allCves = []
  if (scrapedCves):
    allCves = allCves + scrapedCves
  if (cves):
    allCves = allCves + cves

  allCves = clean_cves(allCves)
  if allCves:
    store_article(item, allCves)
  else:
    noCveFound += 1
    #store_article(item, [])


def scrapeCves(newsUrl):
  cves = []
  print("Scraping URL {}".format(newsUrl))
  response = requests.get(newsUrl)
  if (response.status_code > 199 and response.status_code<309):
    result = cvePattern.findall(str(response.content))
    cves = clean_cves(result)
    if (cves):
      print("Scraped CVEs: {}".format(cves))
  #time.sleep(1.0)
  return cves


def store_article(item, result):
  global found, stored, noCveFound, blacklisted, responseCodes, duplicates
  if (len(result) > 0):
    print ("Storing article for CVEs: {}".format(result))
  else:
    print ("Storing article without CVEs.")
    noCveFound += 1

  headers = {"api_key": newsserviceApikey}
  data = {
    "cvesMentioned": result,
    "name": item["name"],
    "datePublished": item["datePublished"],
    "dateRetrieved": datetime.datetime.utcnow().isoformat(),
    "description": item["description"],
    "provider": item["provider"][0]["name"],
    "url": item["url"],
    "sourceType": "news",
    "lang": lang,
    "region": region
  }
  try:
    response = requests.post(newsserviceUrl, headers=headers, json=data)
    if (response.status_code < 200 or response.status_code>308):
      # increase counter for status code:
      responseCodes[response.status_code] = responseCodes.get(response.status_code, 0) + 1
      if ("url dup key" in str(response.content)):
        duplicates += 1
        print("...skipped as duplicate.")
      else:
        print("HTTP response code {}: {}".format(
          response.status_code, str(response.content)))
    else:
      print("...OK ({})".format(response.status_code))
      stored += 1
  except requests.exceptions.RequestException as e:
    raise SystemExit(e)

def main():
  offset=0
  while offset < 501:
    search_page(offset)
    offset += 100
    time.sleep(2.0)

def search_page(offset):
  global found
  print("Running news search: market '{}', freshness '{}', search-term '{}', offset {}".format(
    mkt, freshness, search_term, offset
  ))
  headers = {"Ocp-Apim-Subscription-Key": subscription_key}
  params = {"q": search_term,
    "mkt": mkt,
    "category": "ScienceAndTechnology",
    "count": 100,
    "offset": offset,
    "freshness": freshness}
  response = requests.get(search_url, headers=headers, params=params)
  response.raise_for_status()
  search_results = response.json()

  news = json.loads(response.content)
  #print(json.dumps(news, indent=4, sort_keys=True))

  found += len(news["value"])
  print("Found {} articles with offset {}.\n".format(
    len(news["value"]), offset
  ))

  for item in news["value"]:
    process_item(item)

  print("\nSummary: \nFound: {}, Stored: {}, Skipped as Duplicates: {}, Blacklisted: {}, No CVEs found: {}\n".format(
    found, stored, duplicates, blacklisted, noCveFound))
  if (len(responseCodes)>0):
    print("HTTP error responses (code: count):")
    for code in responseCodes:
      print("{}: {}".format(code, responseCodes[code]))


### Main:
if __name__ == "__main__":
    main()




